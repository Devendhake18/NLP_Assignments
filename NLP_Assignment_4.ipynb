{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`Name:` **Deven Dhake**  \n",
        "`PRN:` **22070126033**  \n",
        "`Class:` **AIML A2**  \n",
        "`Assignment No:` **4**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_WuxAHxGY_6C"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Import Necessary Libraries\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from rouge import Rouge\n",
        "import os\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mCIhYCOdY_6E"
      },
      "outputs": [],
      "source": [
        "# Define the BiLSTM model for text summarization\n",
        "class BiLSTMSummarizer(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super(BiLSTMSummarizer, self).__init__()\n",
        "        # Embedding layer to convert input words to word embeddings\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        # LSTM encoder with bidirectionality to capture context from both directions\n",
        "        self.encoder = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
        "        \n",
        "        # Decoder with LSTM, input is the output from the encoder (concatenated hidden states)\n",
        "        self.decoder = nn.LSTM(embedding_dim, hidden_dim * 2, batch_first=True)\n",
        "        \n",
        "        # Fully connected layer to map the hidden states to the vocabulary size (output)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "\n",
        "    # Forward pass through the model\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        batch_size = src.shape[0]  # Get batch size\n",
        "        trg_len = trg.shape[1]  # Get the length of the target sequence\n",
        "        trg_vocab_size = self.fc.out_features  # Get the output vocabulary size\n",
        "\n",
        "        # Initialize the output tensor with zeros (batch_size, trg_len, vocab_size)\n",
        "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(src.device)\n",
        "\n",
        "        # Pass the source sentence through the embedding layer\n",
        "        embedded = self.embedding(src)\n",
        "\n",
        "        # Pass the embeddings through the bidirectional LSTM encoder\n",
        "        enc_output, (hidden, cell) = self.encoder(embedded)\n",
        "\n",
        "        # Combine the hidden states from both directions (concatenate)\n",
        "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1).unsqueeze(0)\n",
        "        cell = torch.cat((cell[-2,:,:], cell[-1,:,:]), dim=1).unsqueeze(0)\n",
        "\n",
        "        # Start decoding with the first token (usually <sos>)\n",
        "        input = trg[:, 0]\n",
        "\n",
        "        # Loop over each time step in the target sequence\n",
        "        for t in range(1, trg_len):\n",
        "            input_embedded = self.embedding(input).unsqueeze(1)  # Embed the current input token\n",
        "            output, (hidden, cell) = self.decoder(input_embedded, (hidden, cell))  # Decode one step\n",
        "            prediction = self.fc(output.squeeze(1))  # Pass decoder output through fully connected layer\n",
        "\n",
        "            outputs[:, t] = prediction  # Store the prediction at current time step\n",
        "\n",
        "            # Use teacher forcing (feeding correct output token back into the model)\n",
        "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
        "            top1 = prediction.argmax(1)  # Get the predicted token\n",
        "            input = trg[:, t] if teacher_force else top1  # Decide whether to use teacher forcing or not\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gBEvTN3KY_6F"
      },
      "outputs": [],
      "source": [
        "# Dataset class to prepare data for summarization\n",
        "class SummarizationDataset(Dataset):\n",
        "    def __init__(self, articles, summaries, vocab, max_length=100):\n",
        "        self.articles = articles  # List of articles (source text)\n",
        "        self.summaries = summaries  # List of summaries (target text)\n",
        "        self.vocab = vocab  # Vocabulary mapping\n",
        "        self.max_length = max_length  # Maximum sequence length for padding/truncating\n",
        "\n",
        "    # Return the length of the dataset\n",
        "    def __len__(self):\n",
        "        return len(self.articles)\n",
        "\n",
        "    # Return a sample of data (article, summary) as tensors\n",
        "    def __getitem__(self, idx):\n",
        "        article = self.articles[idx]\n",
        "        summary = self.summaries[idx]\n",
        "\n",
        "        # Convert article to a list of token indices\n",
        "        article_indices = [self.vocab['<sos>']] + [self.vocab.get(token, self.vocab['<unk>']) for token in article][:self.max_length-2] + [self.vocab['<eos>']]\n",
        "        summary_indices = [self.vocab['<sos>']] + [self.vocab.get(token, self.vocab['<unk>']) for token in summary][:self.max_length-2] + [self.vocab['<eos>']]\n",
        "\n",
        "        # Pad sequences to max_length\n",
        "        article_indices = article_indices + [self.vocab['<pad>']] * (self.max_length - len(article_indices))\n",
        "        summary_indices = summary_indices + [self.vocab['<pad>']] * (self.max_length - len(summary_indices))\n",
        "\n",
        "        return torch.tensor(article_indices), torch.tensor(summary_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tgn_dCLbY_6H"
      },
      "outputs": [],
      "source": [
        "# Load the dataset from a CSV file (replace with your actual file path and column names)\n",
        "file_path = r\"D:\\New folder\\CODDING STUFF\\Sem 5\\NLPL\\Assignment 4\\Hindi Summarization-20241003\\hindi_news_dataset.csv\"\n",
        "def load_data(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    return df['Headline'].tolist(), df['Content'].tolist()  # Replace 'Headline' and 'Content' with actual column names\n",
        "\n",
        "# Tokenize text using word_tokenize from nltk\n",
        "def tokenize(text):\n",
        "    return word_tokenize(text.lower())  # Tokenize and lowercase the text\n",
        "\n",
        "# Build vocabulary from the dataset\n",
        "def build_vocab(texts, min_freq=2):\n",
        "    word_freq = Counter()  # Count word frequencies\n",
        "    for text in texts:\n",
        "        word_freq.update(text)\n",
        "\n",
        "    # Initialize special tokens\n",
        "    vocab = {'<pad>': 0, '<unk>': 1, '<sos>': 2, '<eos>': 3}\n",
        "    \n",
        "    # Add words with frequency >= min_freq\n",
        "    for word, freq in word_freq.items():\n",
        "        if freq >= min_freq:\n",
        "            vocab[word] = len(vocab)\n",
        "\n",
        "    return vocab, {v: k for k, v in vocab.items()}  # Return word2idx and idx2word mappings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lHfREZHwY_6I"
      },
      "outputs": [],
      "source": [
        "# Load and tokenize the articles and summaries\n",
        "articles, summaries = load_data(file_path)\n",
        "tokenized_articles = [tokenize(article) for article in articles]\n",
        "tokenized_summaries = [tokenize(summary) for summary in summaries]\n",
        "\n",
        "# Build vocabulary\n",
        "vocab, inv_vocab = build_vocab(tokenized_articles + tokenized_summaries)\n",
        "\n",
        "# Split the data into training, validation, and test sets\n",
        "train_articles, test_articles, train_summaries, test_summaries = train_test_split(tokenized_articles, tokenized_summaries, test_size=0.2, random_state=42)\n",
        "train_articles, val_articles, train_summaries, val_summaries = train_test_split(train_articles, train_summaries, test_size=0.1, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CP18TCZfY_6J"
      },
      "outputs": [],
      "source": [
        "# Create datasets using the tokenized data and vocab\n",
        "train_dataset = SummarizationDataset(train_articles, train_summaries, vocab, max_length=50)\n",
        "val_dataset = SummarizationDataset(val_articles, val_summaries, vocab, max_length=50)\n",
        "test_dataset = SummarizationDataset(test_articles, test_summaries, vocab, max_length=50)\n",
        "\n",
        "# Create data loaders to feed data in batches\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-X7pLGHsY_6J"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Initialize model and hyperparameters\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Use GPU if available\n",
        "\n",
        "vocab_size = len(vocab)   # Size of the vocabulary\n",
        "embedding_dim = 300       # Size of word embeddings\n",
        "hidden_dim = 512          # Size of LSTM hidden state\n",
        "output_dim = vocab_size   # Output size, generally the size of the vocabulary\n",
        "\n",
        "# Initialize the BiLSTM model and move it to the device (GPU/CPU)\n",
        "model = BiLSTMSummarizer(vocab_size, embedding_dim, hidden_dim, output_dim).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UZGJGt5-Y_6K"
      },
      "outputs": [],
      "source": [
        "# Training function\n",
        "def train(model, iterator, optimizer, criterion, device, clip=1, teacher_forcing_ratio=0.5):\n",
        "    model.train()  # Set model to training mode\n",
        "    epoch_loss = 0\n",
        "    for batch in tqdm(iterator, desc=\"Training\"):  # Iterate over batches\n",
        "        src, trg = batch\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "        optimizer.zero_grad()  # Clear gradients\n",
        "        output = model(src, trg, teacher_forcing_ratio)  # Forward pass\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[:, 1:].reshape(-1, output_dim)  # Reshape output for loss calculation\n",
        "        trg = trg[:, 1:].reshape(-1)  # Flatten target sequence\n",
        "\n",
        "        loss = criterion(output, trg)  # Calculate loss\n",
        "        loss.backward()  # Backpropagate\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)  # Clip gradients to avoid exploding gradient\n",
        "\n",
        "        optimizer.step()  # Update parameters\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "sUtmmZLIY_6L"
      },
      "outputs": [],
      "source": [
        "# Evaluation function\n",
        "def evaluate(model, iterator, criterion, device):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation\n",
        "        for batch in tqdm(iterator, desc=\"Evaluating\"):\n",
        "            src, trg = batch\n",
        "            src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "            output = model(src, trg, 0)  # Turn off teacher forcing during evaluation\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[:, 1:].reshape(-1, output_dim)  # Reshape output for loss calculation\n",
        "            trg = trg[:, 1:].reshape(-1)  # Flatten target sequence\n",
        "\n",
        "            loss = criterion(output, trg)  # Calculate loss\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "dFZaxsetY_6L"
      },
      "outputs": [],
      "source": [
        "def beam_search(model, src, vocab, inv_vocab, beam_width=3, max_length=50, min_length=10, device='gpu'):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Embedding the input sequence\n",
        "        embedded = model.embedding(src)  # shape: (batch_size, seq_len, embedding_dim)\n",
        "        enc_output, (hidden, cell) = model.encoder(embedded)  # LSTM encoder output\n",
        "\n",
        "        # In case of bi-directional LSTM, combine the hidden states\n",
        "        if model.encoder.bidirectional:\n",
        "            hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)  # shape: (batch_size, hidden_dim)\n",
        "            cell = torch.cat((cell[-2, :, :], cell[-1, :, :]), dim=1)        # shape: (batch_size, hidden_dim)\n",
        "        else:\n",
        "            hidden = hidden[-1, :, :]  # Take the last layer if not bi-directional\n",
        "            cell = cell[-1, :, :]      # Take the last layer if not bi-directional\n",
        "\n",
        "        # Now we process one sequence at a time, so set batch size to 1\n",
        "        hidden = hidden.unsqueeze(0)  # shape: (1, batch_size, hidden_dim)\n",
        "        cell = cell.unsqueeze(0)      # shape: (1, batch_size, hidden_dim)\n",
        "\n",
        "        # Initialize the beam with the start-of-sequence token\n",
        "        beam = [([vocab['<sos>']], 0, hidden[:, 0:1, :], cell[:, 0:1, :])]  # Start with one sequence\n",
        "        complete_hypotheses = []\n",
        "\n",
        "        # Perform beam search\n",
        "        for t in range(max_length):\n",
        "            new_beam = []\n",
        "            for seq, score, hidden, cell in beam:\n",
        "                # If end-of-sequence token is reached and length is >= min_length, add to complete hypotheses\n",
        "                if seq[-1] == vocab['<eos>'] and len(seq) >= min_length:\n",
        "                    complete_hypotheses.append((seq, score))\n",
        "                    continue\n",
        "\n",
        "                # Prepare the input for the decoder (last predicted token)\n",
        "                input = torch.LongTensor([seq[-1]]).unsqueeze(0).to(device)  # shape: (1, 1)\n",
        "                input_embedded = model.embedding(input)  # shape: (1, 1, embedding_dim)\n",
        "\n",
        "                # Pass through the decoder with the current hidden and cell states\n",
        "                output, (hidden, cell) = model.decoder(input_embedded, (hidden, cell))  # hidden, cell are (1, 1, hidden_dim)\n",
        "                predictions = model.fc(output.squeeze(1))  # shape: (1, vocab_size)\n",
        "\n",
        "                # Prevent EOS if sequence is shorter than minimum length\n",
        "                if len(seq) < min_length:\n",
        "                    predictions[0][vocab['<eos>']] = float('-inf')\n",
        "\n",
        "                # Get top beam_width predictions\n",
        "                top_preds = torch.topk(predictions, beam_width, dim=1)\n",
        "\n",
        "                # For each top prediction, extend the sequence and update the beam\n",
        "                for i in range(beam_width):\n",
        "                    new_seq = seq + [top_preds.indices[0][i].item()]\n",
        "                    new_score = score - top_preds.values[0][i].item()  # Negative log probability\n",
        "                    new_hidden = hidden.clone()\n",
        "                    new_cell = cell.clone()\n",
        "                    new_beam.append((new_seq, new_score, new_hidden, new_cell))\n",
        "\n",
        "            # Sort by score and keep top beam_width sequences\n",
        "            beam = sorted(new_beam, key=lambda x: x[1])[:beam_width]\n",
        "\n",
        "            if len(complete_hypotheses) >= beam_width:\n",
        "                break\n",
        "\n",
        "        # Sort and return the best sequence\n",
        "        complete_hypotheses = sorted(complete_hypotheses, key=lambda x: x[1])\n",
        "        if complete_hypotheses:\n",
        "            best_seq = complete_hypotheses[0][0]\n",
        "        else:\n",
        "            best_seq = beam[0][0]\n",
        "\n",
        "    # Convert sequence of indices back to words\n",
        "    return [inv_vocab[idx] for idx in best_seq if idx not in [vocab['<sos>'], vocab['<eos>'], vocab['<pad>']]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "iYMa8KYbY_6M"
      },
      "outputs": [],
      "source": [
        "# Save model function\n",
        "def save_model(model, vocab, filepath):\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'vocab': vocab\n",
        "    }, filepath)\n",
        "    print(f\"Model saved to {'Copy_of_Hindi_Summarization_Beam_Search copy.ipynb'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "fCI_lkWhY_6M"
      },
      "outputs": [],
      "source": [
        "# Define optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=vocab['<pad>'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "WyyssAHKY_6M"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2087/2087 [37:08<00:00,  1.07s/it]\n",
            "Evaluating: 100%|██████████| 232/232 [00:59<00:00,  3.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 01\n",
            "\tTrain Loss: 5.151\n",
            "\t Val. Loss: 5.922\n",
            "Model saved to Copy_of_Hindi_Summarization_Beam_Search copy.ipynb\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2087/2087 [37:06<00:00,  1.07s/it]\n",
            "Evaluating: 100%|██████████| 232/232 [01:01<00:00,  3.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 02\n",
            "\tTrain Loss: 3.127\n",
            "\t Val. Loss: 4.733\n",
            "Model saved to Copy_of_Hindi_Summarization_Beam_Search copy.ipynb\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2087/2087 [36:12<00:00,  1.04s/it]\n",
            "Evaluating: 100%|██████████| 232/232 [00:57<00:00,  4.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 03\n",
            "\tTrain Loss: 2.224\n",
            "\t Val. Loss: 4.046\n",
            "Model saved to Copy_of_Hindi_Summarization_Beam_Search copy.ipynb\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2087/2087 [36:19<00:00,  1.04s/it]\n",
            "Evaluating: 100%|██████████| 232/232 [00:57<00:00,  4.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 04\n",
            "\tTrain Loss: 1.744\n",
            "\t Val. Loss: 3.587\n",
            "Model saved to Copy_of_Hindi_Summarization_Beam_Search copy.ipynb\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2087/2087 [37:09<00:00,  1.07s/it]\n",
            "Evaluating: 100%|██████████| 232/232 [01:01<00:00,  3.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 05\n",
            "\tTrain Loss: 1.435\n",
            "\t Val. Loss: 3.243\n",
            "Model saved to Copy_of_Hindi_Summarization_Beam_Search copy.ipynb\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2087/2087 [37:09<00:00,  1.07s/it]\n",
            "Evaluating: 100%|██████████| 232/232 [01:02<00:00,  3.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 06\n",
            "\tTrain Loss: 1.211\n",
            "\t Val. Loss: 3.036\n",
            "Model saved to Copy_of_Hindi_Summarization_Beam_Search copy.ipynb\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2087/2087 [36:58<00:00,  1.06s/it]\n",
            "Evaluating: 100%|██████████| 232/232 [00:58<00:00,  3.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 07\n",
            "\tTrain Loss: 1.060\n",
            "\t Val. Loss: 2.881\n",
            "Model saved to Copy_of_Hindi_Summarization_Beam_Search copy.ipynb\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2087/2087 [36:51<00:00,  1.06s/it]\n",
            "Evaluating: 100%|██████████| 232/232 [01:00<00:00,  3.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 08\n",
            "\tTrain Loss: 0.934\n",
            "\t Val. Loss: 2.765\n",
            "Model saved to Copy_of_Hindi_Summarization_Beam_Search copy.ipynb\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2087/2087 [36:47<00:00,  1.06s/it]\n",
            "Evaluating: 100%|██████████| 232/232 [00:59<00:00,  3.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 09\n",
            "\tTrain Loss: 0.845\n",
            "\t Val. Loss: 2.638\n",
            "Model saved to Copy_of_Hindi_Summarization_Beam_Search copy.ipynb\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2087/2087 [36:51<00:00,  1.06s/it]\n",
            "Evaluating: 100%|██████████| 232/232 [00:57<00:00,  4.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 10\n",
            "\tTrain Loss: 0.769\n",
            "\t Val. Loss: 2.568\n",
            "Model saved to Copy_of_Hindi_Summarization_Beam_Search copy.ipynb\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "num_epochs = 10\n",
        "best_val_loss = float('inf')\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(model, train_loader, optimizer, criterion, device)\n",
        "    val_loss = evaluate(model, val_loader, criterion, device)\n",
        "    print(f'Epoch: {epoch+1:02}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\t Val. Loss: {val_loss:.3f}')\n",
        "\n",
        "    # Save model if validation loss improves\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        save_model(model, vocab, 'best_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "3Di3pobeY_6N"
      },
      "outputs": [],
      "source": [
        "# Load model function\n",
        "def load_model(filepath, device):\n",
        "    checkpoint = torch.load(filepath, map_location=device)\n",
        "    vocab = checkpoint['vocab']\n",
        "    model = BiLSTMSummarizer(vocab_size, embedding_dim, hidden_dim, output_dim).to(device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    return model, checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "DPyf-nlSY_6N",
        "outputId": "4bb48d10-785b-4019-c8b5-dddc650e6a43"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\deven\\AppData\\Local\\Temp\\ipykernel_3816\\114305855.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(filepath, map_location=device)\n",
            "Evaluating: 100%|██████████| 580/580 [02:25<00:00,  4.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 2.572\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating summaries: 100%|██████████| 580/580 [03:03<00:00,  3.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROUGE scores:\n",
            "{'rouge-1': {'r': 0.8083014297750529, 'p': 0.8337258642152, 'f': 0.8193316370463417}, 'rouge-2': {'r': 0.728746530253046, 'p': 0.7376144754509335, 'f': 0.7327716971193842}, 'rouge-l': {'r': 0.786234929785507, 'p': 0.8089017092229435, 'f': 0.7960673826807474}}\n"
          ]
        }
      ],
      "source": [
        "# Load the best model for testing\n",
        "best_model, _ = load_model('best_model.pth', device)\n",
        "\n",
        "# Test the model\n",
        "test_loss = evaluate(best_model, test_loader, criterion, device)\n",
        "print(f'Test Loss: {test_loss:.3f}')\n",
        "\n",
        "# Evaluate using ROUGE score\n",
        "rouge = Rouge()\n",
        "best_model.eval()\n",
        "predictions = []\n",
        "references = []\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader, desc=\"Generating summaries\"):\n",
        "        src, trg = batch\n",
        "        src = src.to(device)\n",
        "        pred = beam_search(best_model, src, vocab, inv_vocab, min_length=10, device=device)  # Set minimum length\n",
        "        predictions.extend([' '.join(pred)])\n",
        "        references.extend([' '.join([inv_vocab[idx.item()] for idx in trg[0] if idx.item() not in [vocab['<sos>'], vocab['<eos>'], vocab['<pad>']]])])\n",
        "\n",
        "# Ensure all predictions meet the minimum length\n",
        "min_length = 10  # Set this to your desired minimum length\n",
        "predictions = [p if len(p.split()) >= min_length else p + ' ' + ' '.join(['<pad>'] * (min_length - len(p.split()))) for p in predictions]\n",
        "\n",
        "scores = rouge.get_scores(predictions, references, avg=True)\n",
        "print(\"ROUGE scores:\")\n",
        "print(scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "Y68PKEeYY_6N",
        "outputId": "92d0777f-cd84-4b63-efb5-8a0e8a4b2447"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading pre-trained model...\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading pre-trained model...\")\n",
        "trained_model, checkpoint = load_model('best_model.pth', device)\n",
        "vocab = checkpoint['vocab']\n",
        "inv_vocab = {v: k for k, v in vocab.items()}\n",
        "trained_model = trained_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "CMv-l-VmY_6O"
      },
      "outputs": [],
      "source": [
        "# Modified Summarization bot\n",
        "def summarize_text(model, vocab, inv_vocab, text, max_length=100, min_length=10, beam_width=3, device='cpu', debug=False):\n",
        "    model.eval()\n",
        "    tokens = tokenize(text)[:max_length]\n",
        "    indices = [vocab['<sos>']] + [vocab.get(token, vocab['<unk>']) for token in tokens] + [vocab['<eos>']]\n",
        "    src = torch.LongTensor(indices).unsqueeze(0).to(device)\n",
        "\n",
        "    summary = beam_search(model, src, vocab, inv_vocab, beam_width, max_length, min_length, device)\n",
        "\n",
        "    if debug:\n",
        "        print(\"Input tokens:\", tokens)\n",
        "        print(\"Input indices:\", indices)\n",
        "        print(\"Generated indices:\", [vocab[word] for word in summary])\n",
        "        print(\"Summary length:\", len(summary))\n",
        "\n",
        "    return ' '.join(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "-6HLvvrkY_6O",
        "outputId": "b4210c5f-ccef-496b-9339-27f7b8ec863a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input tokens: ['ऑस्ट्रेलिया', 'ने', 'ब्लूमफोनटीन', 'में', 'पहले', 'वनडे', 'में', 'दक्षिण', 'अफ्रीका', 'को', '3-विकेट', 'से', 'हरा', 'दिया।', 'यह', '12', 'वर्षों', 'में', 'दक्षिण', 'अफ्रीका', 'के', 'खिलाफ', 'उसकी', 'धरती', 'पर', 'ऑस्ट्रेलिया', 'की', 'पहली', 'वनडे', 'जीत', 'है।', 'ऑस्ट्रेलिया', 'का', 'स्कोर', '16.3', 'ओवर', 'में', '113/7', 'था', 'लेकिन', 'मार्नस', 'लबुशेन', 'और', 'ऐश्टन', 'एगर', 'की', '112', '*', 'रनों', 'की', 'साझेदारी', 'की', 'बदौलत', 'उसने', '40.2', 'ओवर', 'में', 'लक्ष्य', 'हासिल', 'कर', 'लिया।']\n",
            "Input indices: [2, 3351, 83, 29389, 10, 1276, 3352, 10, 3184, 965, 76, 29390, 37, 3192, 27649, 229, 605, 489, 10, 3184, 965, 12, 323, 431, 3771, 98, 3351, 8, 575, 3352, 2706, 27646, 3351, 24, 3490, 29391, 3396, 10, 29392, 28, 2458, 3769, 3770, 73, 29393, 29394, 8, 10147, 3628, 8210, 8, 11848, 8, 3884, 4727, 29395, 3396, 10, 1983, 3806, 103, 27891, 3]\n",
            "Generated indices: [3352, 490, 3358, 1958, 10, 1276, 1243, 10, 3351, 12, 323, 1243, 10, 5887, 37, 6826, 496, 59, 3728, 3701, 29189, 76, 3578, 1862, 27649, 3351, 83, 1032, 852, 3351, 10, 3351, 8, 3434, 10, 3408, 3556, 83, 8519, 3396, 10, 3351, 76, 3192, 478, 27647, 3351, 83, 83, 1032, 852, 87, 8, 83, 83, 211, 852, 3351, 8]\n",
            "Summary length: 59\n",
            "Generated Summary:\n",
            "वनडे विश्व कप 2023 में पहले मैच में ऑस्ट्रेलिया के खिलाफ मैच में गलती से बचने करते हुए ओपनर फखर ज़मान को आउट करार दिया। ऑस्ट्रेलिया ने बताया कि ऑस्ट्रेलिया में ऑस्ट्रेलिया की पारी में पाकिस्तानी बल्लेबाज़ों ने उस ओवर में ऑस्ट्रेलिया को हरा दिया था। ऑस्ट्रेलिया ने ने बताया कि भारत की ने ने कहा कि ऑस्ट्रेलिया की\n",
            "Summary length: 59\n"
          ]
        }
      ],
      "source": [
        "# Example usage of the summarization bot\n",
        "input_text = \"ऑस्ट्रेलिया ने ब्लूमफोनटीन में पहले वनडे में दक्षिण अफ्रीका को 3-विकेट से हरा दिया। यह 12 वर्षों में दक्षिण अफ्रीका के खिलाफ उसकी धरती पर ऑस्ट्रेलिया की पहली वनडे जीत है। ऑस्ट्रेलिया का स्कोर 16.3 ओवर में 113/7 था लेकिन मार्नस लबुशेन और ऐश्टन एगर की 112* रनों की साझेदारी की बदौलत उसने 40.2 ओवर में लक्ष्य हासिल कर लिया।\"\n",
        "summary = summarize_text(trained_model, vocab, inv_vocab, input_text, min_length=10, device=device, debug=True)\n",
        "print(\"Generated Summary:\")\n",
        "print(summary)\n",
        "print(\"Summary length:\", len(summary.split()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Rouge Score:\n",
        "\n",
        "ROUGE-1:\n",
        "\n",
        "    Recall: 0.808\n",
        "    Precision: 0.834\n",
        "    F1-score: 0.819\n",
        "\n",
        "ROUGE-2:\n",
        "\n",
        "    Recall: 0.729\n",
        "    Precision: 0.738\n",
        "    F1-score: 0.733\n",
        "\n",
        "ROUGE-L:\n",
        "\n",
        "    Recall: 0.786\n",
        "    Precision: 0.809\n",
        "    F1-score: 0.796"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pytorch_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
